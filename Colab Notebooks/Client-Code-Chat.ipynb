{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCh0NDcihk7hy1HDwGTnlp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**CLIENT** Notebook for interacting with Mistral instance running in the \"Mistral-core.ipynb\" notebook\n","\n","Substitute URL_GENERATED_BY_ngrok with the URL generated when running ngrok in the \"Mistral-core.ipynb\" notebook"],"metadata":{"id":"SgAYjBAWgTsk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrOKSAHprupR"},"outputs":[],"source":["import os\n","\n","url_ollama_host = \"URL_GENERATED_BY_ngrok\"\n","\n","os.environ[\"OLLAMA_HOST\"] = url_ollama_host"]},{"cell_type":"markdown","source":["Install here Ollama (see \"Mistral-core.ipynb\" notebook for clarification about the following command"],"metadata":{"id":"5nHd_gHXgnQe"}},{"cell_type":"code","source":["!curl -fsSL https://ollama.com/install.sh | sed 's#https://ollama.com/download#https://github.com/jmorganca/ollama/releases/download/v0.1.27#' | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DRRzUsoEVYE","executionInfo":{"status":"ok","timestamp":1711543857345,"user_tz":-60,"elapsed":5276,"user":{"displayName":"Massimiliano GARDA","userId":"05782795210872681522"}},"outputId":"36a905d7-5ac1-49dd-8543-c911ad54d61e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Downloading ollama...\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}]},{"cell_type":"markdown","source":["Launch the command to pull Mistral"],"metadata":{"id":"kytDTpg3gz8H"}},{"cell_type":"code","source":["!ollama pull mistral"],"metadata":{"id":"1rhxe_QW4osA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install dependencies"],"metadata":{"id":"xDUgk_eVhQyT"}},{"cell_type":"code","source":["!pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface"],"metadata":{"id":"Wb_pe2C0tvcG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install llama-index-llms-openai"],"metadata":{"id":"e1zpOABYqS8B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a directory \"data\" and upload inside the description documents of Atomic Data Services (e.g., JSON OpenAPI specification documents)"],"metadata":{"id":"wB2JeMF7hcGI"}},{"cell_type":"code","source":["!mkdir ./data"],"metadata":{"id":"17PLlPTYu3tD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the index for **RAG** modality.\n","Remember to set the base_url variable to the URL generated by ngrok so the core of Mistral is served on the other notebook with GPU RAM.\n","\n","A retriever is also configured to test the query interaction mode (not detailed in the notebook) with respect to the chat mode"],"metadata":{"id":"vjnpMZwRhpGw"}},{"cell_type":"code","source":["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, get_response_synthesizer\n","from llama_index.core.retrievers import VectorIndexRetriever\n","from llama_index.core.query_engine import RetrieverQueryEngine\n","from llama_index.core.embeddings import resolve_embed_model\n","from llama_index.llms.ollama import Ollama\n","\n","documents = SimpleDirectoryReader(\"data\").load_data()\n","\n","# bge-m3 embedding model\n","Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n","\n","# ollama\n","Settings.llm = Ollama(model=\"mistral\", request_timeout=300.0, base_url=url_ollama_host)\n","\n","index = VectorStoreIndex.from_documents(\n","    documents,\n",")\n","\n","# configure retriever\n","retriever = VectorIndexRetriever(\n","    index=index,\n","    similarity_top_k=4,\n",")\n","\n","# configure response synthesizer\n","response_synthesizer = get_response_synthesizer(\n","    response_mode=\"compact\",\n",")\n"],"metadata":{"id":"KmWToYURvCNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(Settings.llm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5gBxlCvhQxU","executionInfo":{"status":"ok","timestamp":1711547196815,"user_tz":-60,"elapsed":215,"user":{"displayName":"Massimiliano GARDA","userId":"05782795210872681522"}},"outputId":"4379b4fc-7707-4a8c-d0e9-62153d593693"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7e4c1bfe3850> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x7e4d1b84f7f0> completion_to_prompt=<function default_completion_to_prompt at 0x7e4ce91c0160> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None base_url='https://53b5-34-125-224-48.ngrok-free.app' model='mistral' temperature=0.75 context_window=3900 request_timeout=300.0 prompt_key='prompt' additional_kwargs={}\n"]}]},{"cell_type":"markdown","source":["Create a chat store to allow subsequent export operations on the chat content"],"metadata":{"id":"IEXhI_Qwia7q"}},{"cell_type":"code","source":["from llama_index.core.storage.chat_store import SimpleChatStore\n","from llama_index.core.memory import ChatMemoryBuffer\n","\n","chat_store = SimpleChatStore()\n","\n","memory = ChatMemoryBuffer.from_defaults(token_limit=30000,\n","                                        chat_store=chat_store,\n","                                        chat_store_key=\"user1\",)"],"metadata":{"id":"qSLad1uzJFip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat_engine = index.as_chat_engine(\n","    chat_mode=\"context\",\n","    memory=memory\n",")"],"metadata":{"id":"WHddpTeGpX0I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's start the interaction with Mistral"],"metadata":{"id":"BEeOuHrvinK2"}},{"cell_type":"code","source":["input_text = \"\"\"\n","Referring to Collect services from the context, please provide an Answer to the Question below.\n","\n","Question: Within the scope of Collect services please find a read service for reading the content of a CSV file.\n","Answer:\n","\n","\"\"\"\n","\n","response = chat_engine.chat(input_text)\n","\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G60kPhBr3otz","executionInfo":{"status":"ok","timestamp":1711055977144,"user_tz":-60,"elapsed":5805,"user":{"displayName":"Massimiliano GARDA","userId":"05782795210872681522"}},"outputId":"cb83c966-94c9-451c-f072-f22cdb93890e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Based on the context information provided, you can find a read service for retrieving the content of a CSV file by making a GET request to the \"/read-csv\" endpoint in the Collect service for CSV data retrieval. The file path of the CSV file should be passed as a query parameter named \"file\\_path\". Here is an example of how you can make this request using a web browser or a tool like Postman:\n","\n","```\n","GET /read-csv?file_path=/content/data/your_csv_file.csv\n","```\n","\n","This will return the content of the CSV file in the response, which is a binary string format (text/csv). If the file does not exist, you will receive a \"404\" error indicating that the CSV file was not found.\n"]}]},{"cell_type":"markdown","source":["After chatting, persist the chat in a JSON file (if needed)"],"metadata":{"id":"W5Q-7jzOjNU9"}},{"cell_type":"code","source":["chat_store_string = chat_store.persist(persist_path=\"./output_chat.json\")"],"metadata":{"id":"kCu1KzXCvMC-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the following command to reset the chat"],"metadata":{"id":"xz2L6j8AjYN9"}},{"cell_type":"code","source":["chat_engine.reset()"],"metadata":{"id":"qQVdu31kKQah"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**USE OF PromptTemplate CLASS**:\n","To build the prompt, resort to PromptTemplate class: prepare the template (using placeholders) and then, before interacting with the LLM, instantiate it"],"metadata":{"id":"GJ4bMxqujqK0"}},{"cell_type":"code","source":["from llama_index.core import PromptTemplate\n","\n","prompt_template_qt1_string = \"\"\"\\\n","Referring to {atomic_service_type} from the provided context, please Answer the Question below.\n","\n","Question: Within the scope of {atomic_service_type} please find a service for {task}\n","of {input_dataset}. \\n\n","Answer:\n","\"\"\"\n","\n","prompt_template_qt1 = PromptTemplate(prompt_template_qt1_string)"],"metadata":{"id":"6a8_YZjHIPjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inst_prompt_template_qt1 = prompt_template_qt1.format(\n","    atomic_service_type=\"COLLECT services\",\n","    task=\"reading the content\",\n","    input_dataset=\"a database table\")"],"metadata":{"id":"6BCmJoHKIQWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(inst_prompt_template_qt1)"],"metadata":{"id":"uYHJdSgVIZaP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, use this instantiated prompt to feed the LLM (as described before)"],"metadata":{"id":"-fd1KbaWk6yD"}}]}