{"cells":[{"cell_type":"markdown","source":["Choose a T4 environment to leverage GPU RAM for Mistral"],"metadata":{"id":"bGiP9pR2f0PG"}},{"cell_type":"markdown","source":["**DOWNLOAD OLLAMA** (latest version does not work with tunnelling yet, so a prior version is being downloaded)"],"metadata":{"id":"AJiwZ6BVfb-Z"}},{"cell_type":"code","source":["!curl -fsSL https://ollama.com/install.sh | sed 's#https://ollama.com/download#https://github.com/jmorganca/ollama/releases/download/v0.1.27#' | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJR3DU3aEi4l","executionInfo":{"status":"ok","timestamp":1711543817249,"user_tz":-60,"elapsed":6976,"user":{"displayName":"Massimiliano GARDA","userId":"05782795210872681522"}},"outputId":"7e47a083-d026-4034-eafd-d4c7f6d7bb1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Downloading ollama...\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}]},{"cell_type":"markdown","source":["Now, use ngrok to expose Mistral core through HTTP (substitute \"YOUR_ngrok_TOKEN_HERE\" with the own API Token)"],"metadata":{"id":"l0eQA3saf4SG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sE7dnFbkkeuX"},"outputs":[],"source":["!pip install aiohttp pyngrok\n","\n","import os\n","import asyncio\n","\n","# Set LD_LIBRARY_PATH so the system NVIDIA library\n","os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n","\n","async def run_process(cmd):\n","  print('>>> starting', *cmd)\n","  p = await asyncio.subprocess.create_subprocess_exec(\n","      *cmd,\n","      stdout=asyncio.subprocess.PIPE,\n","      stderr=asyncio.subprocess.PIPE,\n","  )\n","\n","  async def pipe(lines):\n","    async for line in lines:\n","      print(line.strip().decode('utf-8'))\n","\n","  await asyncio.gather(\n","      pipe(p.stdout),\n","      pipe(p.stderr),\n","  )\n","\n","#register an account at ngrok.com and create an authtoken and place it here\n","await asyncio.gather(\n","    run_process(['ngrok', 'config', 'add-authtoken','YOUR_ngrok_TOKEN_HERE'])\n",")\n","\n","await asyncio.gather(\n","    run_process(['ollama', 'serve']),\n","    run_process(['ngrok', 'http', '--log', 'stderr', '11434']),\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNwtZ3vTtIdMHvkawzoPbah"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}